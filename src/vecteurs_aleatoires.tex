\chapter{Vecteurs aléatoires}

\label{chapter:vecteurs_aleatoires}

Dans ce chapitre, on considère $(\Omega, \mathcal{A}, P)$ un espace de
probabilité.
Rappelons que $P : (\Omega, \mathcal{A}) \rightarrow [0, 1]$.

\section{Notions et propriétés}

Rappelons qu'une variable aléatoire réelle n'est qu'une fonction mesurable de
l'espace mesurable $(\Omega, \mathcal{A})$ dans l'espace mesurable $(\real,
\borelian{\real})$.

\begin{definition}
	\textbf{Un vecteur aléatoire $X$ dans $\real^{d}$} est un $d$-uplet $(X_{1}, \cdots,
	X_{d})$ de variable aléatoire réelle.

	On note $X = (X_{1}, \cdots, X_{d})$.
\end{definition}

Il en découle immédiatement la proposition suivante.

\begin{proposition}
	$X = (X_{1}, \cdots, X_{d})$ est un vecteur aléatoire ssi $X : (\Omega,
	\mathcal{A}) \rightarrow (\real^{d}, \borelian{\real^{d}})$ est mesurable.
\end{proposition}

\ifdefined\outputproof
\begin{proof}

\end{proof}

Nous définissons alors naturellement la loi d'un vecteur aléatoire.

\begin{definition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire.

	On définit \textbf{la loi de $X$}, noté $P_{X}$ comme la fonction
	\begin{align}
		P_{X} : (\real^{d}, \borelian{\real^{d}}) &\rightarrow [0, 1] \\
		A &\rightarrow P_{X}(A)
	\end{align}

	où
	\begin{align}
		P_{X}(A) & := P(X^{-1}(A)) \\
		&:= P(\GSsetDef{\omega \in \Omega}{X(\omega) \in A}) \\
		& := P(X \in A)
	\end{align}

	On utilisera plus souvent la notation $P(X \in A)$.

	On note alors $X \varFollow P_{X}$.
\end{definition}

\begin{proposition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire.

	Alors $P_{X}$ est une probabilité sur l'espace $(\real^{d},
	\borelian{\real^{d}})$.
\end{proposition}

\ifdefined\outputproof
\begin{proof}

\end{proof}

Comme nous avons fait pour les variables aléatoires réelles dans le chapitre
\ref{chapter:variables_aleatoires_reelles}, nous allons discerner deux
types de vecteurs aléatoires.

\begin{definition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire.

	\begin{itemize}
		\item On dit que $X$ est \textbf{(un vecteur aléatoire) discret} si sa
			loi est discrète.
		\item On dit que $X$ est \textbf{un vecteur aléatoire continue} si sa
			loi est continue.
	\end{itemize}
\end{definition}

\begin{definition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire.
	Les lois des $X_{k}$, $1 \leq k \leq d$, sont appelées \textbf{les lois
	marginales}, et la loi de $X$ est appelée \textbf{loi conjointe}.
\end{definition}

\begin{proposition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire.

	Soit $A \in \borelian{\real^{d}}$.
	Alors

	\begin{equation}
		P_{X_{k}} = P_{X}(\overbrace{\real \cartprod \cdots \cartprod \real}^{d - 1 \text{ fois}},
	A \cartprod \real \cartprod \cdots \cartprod \real)
	\end{equation}
\end{proposition}

\ifdefined\outputproof
\begin{proof}

\end{proof}

\begin{corollary}
	Soit $X = (X, Y)$ un vecteur aléatoire discret.

	Alors $X \varIndependant Y$ ssi les produits des probabilités des lois
	marginales donnent la loi conjointe.
\end{corollary}

\ifdefined\outputproof
\begin{proof}

\end{proof}

Nous pouvons alors généraliser le théorème du transfert vu dans le cas des
variables aléatoires réelles.

\begin{theorem} [Théorème du transfert]
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire.
	Soit $h : \real^{d} \rightarrow \real$ mesurable (pour la tribu borélienne).

	Alors:

	\begin{equation}
		\int_{\Omega} h(X) dP = \int_{\real^{d}} h(x) dP_{X}(x)
	\end{equation}
	\label{theorem:transfert_vecteur_aleatoire}
\end{theorem}

\ifdefined\outputproof
\begin{proof}

\end{proof}

% Page 6 Résumé Justine

\section{Application à l'indépendance}


\section{Calcul de loi}

\section{Espérance et covariance}

\subsection{Espérance d'un vecteur aléatoire}

\begin{definition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire intégrable.

	\textbf{L'espérance de $X$} est le $d$-uplet réel $E(X) := (E(X_{1}),
	\cdots, E(X_{d}))$.
\end{definition}

\begin{proposition}
	Soient $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire intégrable de $(\Omega,
	\mathcal{A})$ dans $(\real^{d}, \borelian{\real^{d}})$ et $A \in
	\matrixSpace{d' \cartprod d}{\real}$

	Alors $Y := A X$ est un vecteur aléatoire intégrable de $(\Omega,
	\mathcal{A})$ dans $(\real^{d'}, \borelian{\real^{d'}})$ et $E(Y) = A .
	E(X)$.
\end{proposition}

\ifdefined\outputproof
\begin{proof}

\end{proof}

\subsection{Covariance d'un vecteur aléatoire}

\begin{definition}
	Soient $X$ et $Y$ deux variables aléatoires de carré intégrable.

	\textbf{La covariance de $X$ et $Y$}, noté $Cov(X, Y)$, est définie par le
	réel $E( (X - E(X)) (Y - E(Y)) )$.
\end{definition}

\begin{proposition}
	Soient $X, Y$ et $Z$ trois variables aléatoires de carré intégrable, et deux
	réels $a$ et $b$.

	\begin{itemize}
		\item $Cov(X, X) = Var(X)$
		\item $Cov(X, Y) = E(XY) - E(X) E(Y)$
		\item $Cov(X, Y) = Cov(Y, X)$
		\item $Cov(aX + bY, Z) = a Cov(X, Z) + b Cov(Y, Z)$
		\item $\abs{Cov(X, Y)} \leq \sqrt{Var(X)} \sqrt{Var(Y)}$
	\end{itemize}
\end{proposition}

\ifdefined\outputproof
\begin{proof}

\end{proof}

\begin{proposition}
	Soient $X_{1}, \cdots, X_{d}$ des variables aléatoires réelles de carré intégrable.

	Alors
	\begin{equation}
		Var(X_{1} + \cdots + X_{d}) = \sum_{i = 1}^{d} Var(X_{i}) + 2
		\sum_{1 \leq j, k \leq d} Cov(X_{j}, X_{k})
	\end{equation}
\end{proposition}

\ifdefined\outputproof
\begin{proof}

\end{proof}

\begin{definition}
	Soient $X, Y$ deux variables aléatoires réelles.

	On dit que \textbf{$X$ et $Y$ sont non corrélés} si $Cov(X, Y) = 0$.
\end{definition}

\begin{remarque}
	Soient $X, Y$ deux variables aléatoires réelles.

	\begin{itemize}
		\item $X$ et $Y$ non corrélés ssi $E(XY) = E(X) E(Y)$ ssi $Var(X + Y) =
			Var(X) + Var(Y)$.
		\item Si $X \varIndependant Y$ alors $X$ et $Y$ sont non corrélés.
	\end{itemize}
\end{remarque}

\begin{definition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire de carré intégrable.

	On appelle \textbf{matrice de covariance de $X$} la matrice $K_{X}$ donnée
	par

	\begin{equation*}
		\left( Cov(X_{i}, X_{k}) \right)_{1 \leq i, k \leq d}
	\end{equation*}

	On a $K_{X} \in \matrixSpace{d}{\real}$.
\end{definition}

\begin{proposition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire de carré intégrable.

	\begin{itemize}
		\item $K_{X}$ est une matrice carrée, symétrique, avec les variances des
			$X_{k}$ sur sa diagonale.
		\item $K_{X}$ diagonale ssi les $X_{k}$ sont deux à deux indépendants.
	\end{itemize}
\end{proposition}

\ifdefined\outputproof
\begin{proof}

\end{proof}

\begin{lemma}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire de carré intégrable.

	Alors $K_{X} = E( (X - E(X)) (X - E(X))^{t})$.
\end{lemma}

\ifdefined\outputproof
\begin{proof}

\end{proof}

\begin{proposition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire de carré intégrable.

	Soit $A \in \matrixSpace{d}{\real}$.

	Alors $Y := A . X$ (resp. $Y = X . A$) est de carré intégrable et $K_{Y} =
	K_{A . X} = A . K_{X} . A^{t}$ (resp $K_{Y} = K_{X . A} = A^{t} . K_{X} . A$)
\end{proposition}

\ifdefined\outputproof
\begin{proof}

\end{proof}

\begin{definition}
	Une matrice carrée $M \in \matrixSpace{d}{\real}$ est dite
	\textbf{semi-définie positive} si pour tout $a \in \real^{d}$, $a^{t} . M .
	a \geq 0$.
\end{definition}

\begin{proposition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire de carré intégrable.

	Alors $K_{X}$ est semi-définie positive.
\end{proposition}

\ifdefined\outputproof
\begin{proof}

\end{proof}

\begin{theorem}
	Soit $\mu \in \real^{d}$ et $K$ une matrice $d \cartprod d$ symétrique,
	semi-définie positive.

	Alors il existe un vecteur aléatoire $X = (X_{1}, \cdots, X_{d})$ tel que
	$E(X) = \mu$ et $K_{X} = K$.
\end{theorem}

\ifdefined\outputproof
\begin{proof}

\end{proof}


\section{Fonctions caractéristiques}

On souhaite caractériser les lois d'un vecteur aléatoire comme nous l'avons déjà
fait avec les variables aléatoires dans le chapitre
\ref{chapter:variables_aleatoires_reelles}.

Nous souhaitons donner des critères permettant de déterminer quelle loi suit un
vecteur aléatoire donné.

\begin{definition}
	Soit $\GSfunction{X}{\Omega}{\real}$ une variable aléatoire.

	On définit \textbf{la fonction caractéristique} de $X$ par

	\begin{align}
		\GSfunction{\phi_{X}}{\real}{\complex} : t \rightarrow
		\phi_{X}(t) & = E(e^{i t X}) \\
		& = \int_{\Omega} e^{it X} dP \\
		& = \int_{\real} e^{it x} dP_{X}
		\label{equation:definition_fonction_caracteristique}
	\end{align}
\end{definition}

\begin{remarque}
	\begin{itemize}
		\item \ref{equation:definition_fonction_caracteristique} est une
			intégrale complexe. On a donc
			\begin{equation}
				\int_{\real}e^{itx} dP_{X} = \int_{\real}cos(tx) + i
				\int_{\real} sin(tx)
			\end{equation}
		\item $\phi_{X}$ est mesurable, et $e^{itx}$ est bornée, donc pour toute
			variable aléatoire réelle, $\phi_{X}$ existe.
		\item Si $X \varFollow \rho$ où $\rho$ est une densité, alors $\phi_{X}(t) = \int_{\real} e^{itx}
			\rho(x) dx$
	\end{itemize}
\end{remarque}

\begin{exemple}
	\begin{itemize}
		\item Bernouilli $\lawBernouilli{p}$: $\phi_{X}(t) = (1 - p) + p e^{it}$
		\item Binomiale $\lawBinomial{n}{p}$: $\phi_{X}(t) = ( (1 - p) + p e^{it})^{n}$
		\item Poisson $\lawPoisson{\lambda}$: $\phi_{X}(t) = e^{\lambda (e^{it} - 1)}$
		\item Exponentielle $\lawExponential{\lambda})$: $\phi_{X}(t) = \frac{\lambda}{\lambda
			- it}$
		\item Cauchy: $\phi_{X}(t) = e^{-\abs{t}}$.
		\item Uniforme: $\phi_{X}(t) = \frac{e^{itb} - e^{ita}}{it (b - a)}$
	\end{itemize}
\end{exemple}

\begin{proposition}
	\begin{itemize}
		\item $\phi_{X}(0) = 1$
		\item $\abs{\phi_{X}(t)} \leq 1$
		\item $\phi_{X}$ est continue
		\item $\phi_{aX + b}(t) = e^{itb} \phi_{X}(at)$
	\end{itemize}
\end{proposition}

\begin{theorem}
	Soit $X$ une variable aléatoire. Alors:

	\begin{itemize}
		\item Si $X \varFollow \lawNormal{0}{1}$, alors $\phi_{X}(t) =
			e^{-\frac{t^{2}}{2}}$.
		\item Si $X \varFollow \lawNormal{\mu}{\sigma^{2}}$, alors $\phi_{X}(t)
			= e^{it \mu - \frac{\sigma^{2}t^{2}}{2}}$.
	\end{itemize}
\end{theorem}

\begin{theorem}
	Soient $X$ et $Y$ deux variables aléatoires réelles. On a

	\begin{equation}
		X \varSameLaw Y \equiv \phi_{X} = \phi_{Y}
	\end{equation}

	En d'autres termes, deux variables aléatoires suivent la même loi si et
	seulement si leur fonction caractéristique sont égales.
\end{theorem}

\begin{theorem}
	Soient $X$ et $Y$ deux variables aléatoires réelle tel que $X
	\varIndependant Y$.
	Alors

	\begin{equation}
		\phi_{X + Y}(t) = \phi_{X}(t) \, \phi_{Y}(t)
	\end{equation}
\end{theorem}

\begin{proposition}
	\begin{itemize}
		\item $E(\abs{X}) < \infty$, alors $\phi_{X} \in \mathcal{C}^{1}$ et
			$E(X) = -i \phi_{X}'(0)$.
		\item Si $E(\abs{X}^{n}) < \infty$, alors $\phi_{X} \in
			\mathcal{C}^{n}$ et $E(X^{n}) = (-i)^{n} {\phi^{n}}_{X}'(0)$.
	\end{itemize}
\end{proposition}

Généralisons maintenant la définition de fonction caractéristique aux vecteurs
aléatoires.

\begin{definition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire.
	On définit \textbf{la fonction caractéristique de $X$} comme la fonction
	\begin{align}
		\phi_{X} : \real^{d} &\rightarrow \complex \\
		u = (u_{1}, \cdots, u_{d}) &\rightarrow E(e^{i(u_{1} X_{1} + \cdots +
		u_{d} X_{d})})
		\label{equation:definition_fonction_caractéristique_vecteur}
	\end{align}
\end{definition}

\begin{theorem}
	Soient $X_{1}, \cdots, X_{d}$ $d$ variables aléatoires réelles.

	Posons $X = (X_{1}, \cdots, X_{d})$.

	Alors, les assertions suivantes sont équivalentes
	\begin{itemize}
		\item Pour tout $u = (u_{1}, \cdots, u_{d}) \in \real^{d}$,
				\begin{equation}
					\phi_{X}(u) = \phi_{X_{1}}(u_{1}) \cdots \phi_{X_{d}}(u_{d})
				\end{equation}
		\item Les $X_{i}$ sont indépendants.
	\end{itemize}
\end{theorem}

\section{Vecteurs gaussiens}

\begin{definition}
	Une variable aléatoire $X$ est dite \textbf{gaussienne} si $X \varFollow
	\lawNormal{\mu}{\sigma^{2}}$ où $\sigma^{2} \geq 0$ avec
	$\lawNormal{\mu}{0} = \delta_{\mu}$.

	On dit que $X$ est \textbf{dégénérée} si $X \varFollow \delta_{\mu}$, ie
	$\sigma = 0$.
\end{definition}

\begin{definition}
	Un vecteur aléatoire $X = (X_{1}, \cdots, X_{d})$ est \textbf{gaussien} si
	toute combinaison linéaire réelle des $X_{i}$ est gaussienne ie pour tout
	$a_{1}, \cdots, a_{d} \in \real$, $\displaystyle \sum_{i = 1}^{d}
	a_{i}X_{i}$ est gaussienne.
\end{definition}

\begin{remarque}
	Si $X = (X_{1}, \cdots, X_{d})$ est un vecteur gaussien, alors chaque
	$X_{i}$ est gaussienne. La réciproque étant fausse si nous n'avons pas
	l'indépendance.
\end{remarque}

\begin{proposition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire.
	\begin{itemize}
		\item Si $X$ est gaussien, alors pour toute matrice $A \in \matrixSpace{d}{\real}$, le vecteur aléatoire $Y := A
			X^{t}$ est gaussien.
		\item Si les $X_{k}$ sont iid de loi gaussienne, alors $X$ est gaussien.
	\end{itemize}
\end{proposition}

\begin{remarque}
	Deux variables aléatoires $X, Y$ suivant une loi $\lawNormal{0}{1}$ ne
	forment jamais un vecteur aléatoire $(X, Y)$ gaussien.
\end{remarque}

\begin{theorem}
	Soit $M \in \matrixSpace{d}{\real}$ symétrique définie semi-positive. Alors
	il existe un vecteur gaussien $X$ tel que $K_{X} = M$ et $E(X) = \mu$.
\end{theorem}

\begin{proposition}
	Soit $X \varFollow \lawNormal{\mu}{K}$ un vecteur gaussien. Les assertions
	suivantes sont équivalentes.

	\begin{itemize}
		\item $X$ a une densité.
		\item $K$ est inversible.
		\item $K$ est définie positive.
	\end{itemize}

	De plus, la densité de $X$ est donné par

	\begin{equation}
		\rho(x) = \frac{1}{\sqrt{2 \pi}^{d}} \frac{1}{\sqrt{det(K)}}
		e^{\frac{-1}{2} (X - \mu) K^{-1} (X - mu)^{t}}
	\end{equation}
\end{proposition}

\begin{proposition}
	La fonction caractéristique d'un vecteur gaussien $X = (X_{1}, \cdots,
	X_{d})$ est donné par

	\begin{equation}
		\phi_{X}(u) = e^{iu E(X)^{t} - \frac{1}{2} u K_{X} u^{t}}
	\end{equation}
	pour tout $u \in \real^{d}$.
\end{proposition}

\begin{corollary}
	La loi d'un vecteur gaussien est déterminée par son espérance et sa matrice
	de covariance.
\end{corollary}

\ifdefined\outputproof
\begin{proof}
	En effet, la fonction caractéristique permet de définir la loi. Comme la
	fonction caractéristique est déterminée par la matrice de covariance et
	l'espace du vecteur gaussien, nous avons que la loi est déterminée par ces
	deux dernières.
\end{proof}

\begin{exemple}
	Soient $X \varFollow \lawNormal{0}{2}$ et $Y \varFollow \lawNormal{1}{3}$
	deux variables aléatoires indépendantes.

	Posons $Z = (X, Y)$.

	On a $Z$ qui est un vecteur gaussien car $X$ et $Y$ sont indépendants, et
	$X$ et $Y$ sont des variables aléatoires gaussiennes..

	On a alors $E(Z) := (E(X), E(Y)) = (0, 1)$ et

	\begin{equation}
		K_{Z} =
		\begin{pmatrix}
			Var(X, X) & Cov(X, Y) \\
			Cov(Y, X) & Var(Y, Y)
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 & 0 \\
			0 & 3
		\end{pmatrix}
	\end{equation}

	On en déduit que $Z \varFollow \lawNormal{E(Z)}{K_{Z}} = \lawNormal{(0,
	1)}{\begin{pmatrix}
			2 & 0 \\
			0 & 3
		\end{pmatrix}
	}$.
\end{exemple}

\begin{proposition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire tel que $X \varFollow
	\lawNormal{\mu}{K}$.

	Alors $Y := A X^{t}$ ($A \in \matrixSpace{d' \cartprod d}{\real}$) suit la
	loi $\lawNormal{A \mu^{t},  A K A^{t}}$.
\end{proposition}

\begin{theorem}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire gaussien.
	Les assertions suivantes sont équivalentes.

	\begin{itemize}
		\item Les $X_{k}$ sont indépendants.
		\item Les $X_{k}$ sont non-corrélés deux à deux.
		\item $K_{X}$ est diagonale.
	\end{itemize}
\end{theorem}
