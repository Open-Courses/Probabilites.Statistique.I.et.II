\chapter{Vecteur aleatoire}

\section{Fonctions caractéristiques}

On souhaite caractériser les lois d'un vecteur aléatoire comme nous l'avons déjà
fait avec les variables aléatoires.

Nous souhaitons donner des critères permettant de déterminer quelle loi suit un
vecteur aléatoire donné.

\begin{definition}
	Soit $\GSfunction{X}{\Omega}{\real}$ une variable aléatoire.

	On définit \textbf{la fonction caractéristique} de $X$ par

	\begin{align}
		\GSfunction{\phi_{X}}{\real}{\complex} : t \rightarrow
		\phi_{X}(t) & = E(e^{i t X}) \\
		& = \int_{\Omega} e^{it X} dP \\
		& = \int_{\real} e^{it x} dP_{X}
		\label{equation:definition_fonction_caracteristique}
	\end{align}
\end{definition}

\begin{remarque}
	\begin{itemize}
		\item \ref{equation:definition_fonction_caracteristique} est une
			intégrale complexe. On a donc
			\begin{equation}
				\int_{\real}e^{itx} dP_{X} = \int_{\real}cos(tx) + i
				\int_{\real} sin(tx)
			\end{equation}
		\item $\phi_{X}$ est mesurable, et $e^{itx}$ est bornée, donc pour toute
			variable aléatoire réelle, $\phi_{X}$ existe.
		\item Si $X \varFollow \rho$, alors $\phi_{X}(t) = \int_{\real} e^{itx}
			\rho(x) dx$
	\end{itemize}
\end{remarque}

\begin{exemple}
	\begin{itemize}
		\item Bernouilli $\lawBernouilli{p}$: $\phi_{X}(t) = (1 - p) + p e^{it}$
		\item Binomiale $\lawBinomial{n}{p}$: $\phi_{X}(t) = ( (1 - p) + p e^{it})^{n}$
		\item Poisson $\lawPoisson{\lambda}$: $\phi_{X}(t) = e^{\lambda (e^{it} - 1)}$
		\item Exponentielle $\lawExponential{\lambda})$: $\phi_{X}(t) = \frac{\lambda}{\lambda
			- it}$
		\item Cauchy: $\phi_{X}(t) = e^{-\abs{t}}$.
		\item Uniforme: $\phi_{X}(t) = \frac{e^{itb} - e^{ita}}{it (b - a)}$
	\end{itemize}
\end{exemple}

\begin{proposition}
	\begin{itemize}
		\item $\phi_{X}(0) = 1$
		\item $\abs{\phi_{X}(t)} \leq 1$
		\item $\phi_{X}$ est continue
		\item $\phi_{aX + b}(t) = e^{itb} \phi_{X}(at)$
	\end{itemize}
\end{proposition}

\begin{theorem}
	Soit $X$ une variable aléatoire. Alors:

	\begin{itemize}
		\item Si $X \varFollow \lawNormal{0}{1}$, alors $\phi_{X}(t) =
			e^{-\frac{t^{2}}{2}}$.
		\item Si $X \varFollow \lawNormal{\mu}{\sigma^{2}}$, alors $\phi_{X}(t)
			= e^{it \mu - \frac{\sigma^{2}t^{2}}{2}}$.
	\end{itemize}
\end{theorem}

\begin{theorem}
	Soient $X$ et $Y$ deux variables aléatoires réelles. On a

	\begin{equation}
		X \varSameLaw Y \equiv \phi_{X} = \phi_{Y}
	\end{equation}

	En d'autres termes, deux variables aléatoires suivent la même loi si et
	seulement si leur fonction caractéristique sont égales.
\end{theorem}

\begin{theorem}
	Soient $X$ et $Y$ deux variables aléatoires réelle tel que $X
	\varIndependant Y$.
	Alors

	\begin{equation}
		\phi_{X + Y}(t) = \phi_{X}(t) \, \phi_{Y}(t)
	\end{equation}
\end{theorem}

\begin{proposition}
	\begin{itemize}
		\item $E(\abs{X}) < \infty$, alors $\phi_{X} \in \mathcal{C}^{1}$ et
			$E(X) = -i \phi_{X}'(0)$.
		\item Si $E(\abs{X}^{n}) < \infty$, alors $\phi_{X} \in
			\mathcal{C}^{n}$ et $E(X^{n}) = (-i)^{n} {\phi^{n}}_{X}'(0)$.
	\end{itemize}
\end{proposition}

Généralisons maintenant la définition de fonction caractéristique aux vecteurs
aléatoires.

\begin{definition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire.
	On définit \textbf{la fonction caractéristique de $X$} comme la fonction

	\begin{align}
		\phi_{X} : \real^{d} &\rightarrow \complex \\
		u = (u_{1}, \cdots, u_{d}) &\rightarrow E(e^{i(u_{1} X_{1} + \cdots +
		u_{d} X_{d})})
		\label{equation:definition_fonction_caractéristique_vecteur}
	\end{align}
\end{definition}

\begin{theorem}
	Soient $X_{1}, \cdots, X_{d}$ $d$ variables aléatoires réelles.

	Posons $X = (X_{1}, \cdots, X_{d})$.

	Alors, les assertions suivantes sont équivalentes
	\begin{itemize}
		\item Pour tout $u = (u_{1}, \cdots, u_{d}) \in \real^{d}$,
				\begin{equation}
					\phi_{X}(u) = \phi_{X}(u_{1}) \cdots \phi_{X}(u_{d})
				\end{equation}
		\item Les $X_{i}$ sont indépendants.
	\end{itemize}
\end{theorem}

\section{Vecteurs gaussiens}

\begin{definition}
	Une variable aléatoire $X$ est dite \textbf{gaussienne} si $X \varFollow
	\lawNormal{\mu}{\sigma^{2}}$ où $\sigma^{2} \geq 0$ avec
	$\lawNormal{\mu}{0} = \delta_{\mu}$.

	On dit que $X$ est \textbf{dégénérée} si $X \varFollow \delta_{mu}$, ie
	$\sigma = 0$.
\end{definition}

\begin{definition}
	Un vecteur aléatoire $X = (X_{1}, \cdots, X_{d})$ est \textbf{gaussien} si
	toute combinaison linéaire réelle des $X_{i}$ est gaussienne ie pour tout
	$a_{1}, \cdots, a_{d} \in \real$, $\displaystyle \sum_{i = 1}^{d}
	a_{i}X_{i}$ est gaussienne.
\end{definition}

\begin{remarque}
	Si $X = (X_{1}, \cdots, X_{d})$ est un vecteur gaussien, alors chaque
	$X_{i}$ est gaussienne. La réciproque étant fausse si nous n'avons pas
	l'indépendance.
\end{remarque}

\begin{proposition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire.
	\begin{itemize}
		\item Si $X$ est gaussien, alors pour toute matrice $A \in \matrixSpace{d}{\real}$, le vecteur aléatoire $Y := A
			X^{t}$ est gaussien.
		\item Si les $X_{k}$ sont iid de loi gaussienne, alors $X$ est gaussien.
	\end{itemize}
\end{proposition}

\begin{remarque}
	Deux variables aléatoires $X, Y$ suivant une loi $\lawNormal{0}{1}$ ne
	forment jamais un vecteur aléatoire $(X, Y)$ gaussien.
\end{remarque}

\begin{theorem}
	Soit $M \in \matrixSpace{d}{\real}$ symétrique définie semi-positive. Alors
	il existe un vecteur gaussien $X$ tel que $K_{X} = M$ et $E(X) = \mu$.
\end{theorem}

\begin{proposition}
	Soit $X \varFollow \lawNormal{\mu}{K}$ un vecteur gaussien. Les assertions
	suivantes sont équivalentes.

	\begin{itemize}
		\item $X$ a une densité.
		\item $K$ est inversible.
		\item $K$ est définie positive.
	\end{itemize}

	De plus, la densité de $X$ est donné par

	\begin{equation}
		\rho(x) = \frac{1}{\sqrt{2 \pi}^{d}} \frac{1}{\sqrt{det(K)}}
		e^{\frac{-1}{2} (X - \mu) K^{-1} (X - mu)^{t}}
	\end{equation}
\end{proposition}

\begin{proposition}
	La fonction caractéristique d'un vecteur gaussien $X = (X_{1}, \cdots,
	X_{d})$ est donné par

	\begin{equation}
		\phi_{X}(u) = e^{iu E(X)^{t} - \frac{1}{2} u K_{X} u^{t}}
	\end{equation}
	pour tout $u \in \real^{d}$.
\end{proposition}

\begin{corollary}
	La loi d'un vecteur gaussien est déterminée par son espérance et sa matrice
	de covariance.
\end{corollary}

\ifdefined\outputproof
\begin{proof}
	En effet, la fonction caractéristique permet de définir la loi. Comme la
	fonction caractéristique est déterminée par la matrice de covariance et
	l'espace du vecteur gaussien, nous avons que la loi est déterminée par ces
	deux dernières.
\end{proof}

\begin{exemple}
	Soient $X \varFollow \lawNormal{0}{2}$ et $Y \varFollow \lawNormal{1}{3}$
	deux variables aléatoires indépendantes.

	Posons $Z = (X, Y)$.

	On a $Z$ qui est un vecteur gaussien car $X$ et $Y$ sont indépendants, et
	$X$ et $Y$ sont des variables aléatoires gaussiennes..

	On a alors $E(Z) := (E(X), E(Y)) = (0, 1)$ et

	\begin{equation}
		K_{Z} =
		\begin{pmatrix}
			Var(X, X) & Cov(X, Y) \\
			Cov(Y, X) & Var(Y, Y)
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 & 0 \\
			0 & 3
		\end{pmatrix}
	\end{equation}

	On en déduit que $Z \varFollow \lawNormal{E(Z)}{K_{Z}} = \lawNormal{(0,
	1)}{\begin{pmatrix}
			2 & 0 \\
			0 & 3
		\end{pmatrix}
	}$.
\end{exemple}

\begin{proposition}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire tel que $X \varFollow
	\lawNormal{\mu}{K}$.

	Alors $Y := A X^{t}$ ($A \in \matrixSpace{d' \cartprod d}{\real}$) suit la
	loi $\lawNormal{A \mu^{t},  A K A^{t}}$.
\end{proposition}

\begin{theorem}
	Soit $X = (X_{1}, \cdots, X_{d})$ un vecteur aléatoire gaussien.
	Les assertions suivantes sont équivalentes.

	\begin{itemize}
		\item Les $X_{k}$ sont indépendants.
		\item Les $X_{k}$ sont non-corrélés deux à deux.
		\item $K_{X}$ est diagonale.
	\end{itemize}
\end{theorem}
